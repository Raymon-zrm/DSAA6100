{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzCoy5664aqy"
      },
      "source": [
        "# PyTorch Regression Tutorial: Predicting California Housing Prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tvc5AJTa5Fir"
      },
      "source": [
        "## Introduction:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0CtkkEJ5IGF"
      },
      "source": [
        "In this tutorial, we will walk through the process of using PyTorch, a popular deep learning framework, to build a regression model for predicting housing prices in California. The dataset we'll be using contains various features like the median income, housing median age, average rooms, etc., for different blocks in California. Our goal is to predict the median housing price for these blocks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKUq9nF15OGp"
      },
      "source": [
        "## Step 1: Import Necessary Libraries\n",
        "Before we begin, we need to import the necessary libraries that will be used throughout this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb_4TnIi_APj"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m无法启动 Kernel。 \n",
            "\u001b[1;31mUnable to start Kernel 'dsaa5021 (Python 3.9.18)' due to a timeout waiting for the ports to get used. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_NkhW7w5WKJ"
      },
      "source": [
        "## Step 2: Load and Preprocess the Data\n",
        "The California housing dataset is a popular dataset for regression tasks. It contains data about housing in California and is used to predict the median house value for California districts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiSaFllg34-N"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "y = y.reshape(-1, 1)  # Convert to 2D array for compatibility with PyTorch\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data to have zero mean and unit variance\n",
        "scaler_X = StandardScaler().fit(X_train)\n",
        "X_train = scaler_X.transform(X_train)\n",
        "X_test = scaler_X.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghwHlW-W5h3J"
      },
      "source": [
        "## Step 3: Prepare Data Loaders\n",
        "Data loaders in PyTorch allow for efficient data loading and batching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPcuLCxN3-Uf"
      },
      "outputs": [],
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "\n",
        "# Create a DataLoader for batching and shuffling\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj_auiKF5ZoF"
      },
      "source": [
        "## Step 4: Define the Neural Network Model\n",
        "We will define a simple feed-forward neural network with three layers for our regression task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LxmtI2m38ib"
      },
      "outputs": [],
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(8, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Atq-6g15emv"
      },
      "source": [
        "Here, we have defined a class MyModel that inherits from nn.Module. This model consists of three linear layers with ReLU activation functions in between."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfBZn7Gc5mTJ"
      },
      "source": [
        "## Step 5: Define Loss Function and Optimizer\n",
        "For our regression task, we will use the Mean Squared Error (MSE) as the loss function. The optimizer we'll use is Stochastic Gradient Descent (SGD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h9XYJvb4FEh"
      },
      "outputs": [],
      "source": [
        "model = MyModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXpPie9E5rOh"
      },
      "source": [
        "## Step 6: Train the Model\n",
        "Now, we will train our model using the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dex1xeSH4IJi",
        "outputId": "bf45d86e-ac68-4ecd-c7b5-ca289275c226"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/100], Loss: 0.6150\n",
            "Epoch [20/100], Loss: 0.3312\n",
            "Epoch [30/100], Loss: 0.1732\n",
            "Epoch [40/100], Loss: 0.3570\n",
            "Epoch [50/100], Loss: 0.3490\n",
            "Epoch [60/100], Loss: 0.2939\n",
            "Epoch [70/100], Loss: 0.3566\n",
            "Epoch [80/100], Loss: 0.3910\n",
            "Epoch [90/100], Loss: 0.1870\n",
            "Epoch [100/100], Loss: 0.2011\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hANoFHeV5uq4"
      },
      "source": [
        "## Step 7: Test the Model\n",
        "After training, it's essential to evaluate the model's performance on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI78oo534RTt",
        "outputId": "452760df-fc9c-4327-a367-3530bd4f8670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Loss: 0.2876\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "    test_outputs = model(X_test_tensor)\n",
        "    test_loss = criterion(test_outputs, y_test_tensor)\n",
        "    print(f'Test Loss: {test_loss.item():.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sp04DzN2v-1"
      },
      "source": [
        "## Step 8: Visualize Predictions\n",
        "After evaluating the model's overall performance, it's often useful to visualize some of the predictions to get a sense of how well the model is doing on individual data points.\n",
        "This code will print the predicted and actual values for the first five samples in the test set. By comparing these values, you can get a sense of how close the model's predictions are to the actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oakBVGTh2QkJ",
        "outputId": "d6050023-abd2-4be5-a6b7-6fbe097e1827"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: 0.58, Actual: 0.48\n",
            "Predicted: 1.14, Actual: 0.46\n",
            "Predicted: 4.53, Actual: 5.00\n",
            "Predicted: 2.57, Actual: 2.19\n",
            "Predicted: 2.93, Actual: 2.78\n"
          ]
        }
      ],
      "source": [
        "# Get predictions for a subset of the test data\n",
        "sample_inputs = X_test_tensor[:5]\n",
        "sample_labels = y_test[:5]\n",
        "sample_outputs = model(sample_inputs)\n",
        "\n",
        "for i in range(len(sample_inputs)):\n",
        "    print(f\"Predicted: {sample_outputs[i][0]:.2f}, Actual: {sample_labels[i][0]:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5s0ZPds5w7h"
      },
      "source": [
        "## Conclusion:\n",
        "In this tutorial, we walked through the process of building a regression model using PyTorch. We used the California housing dataset, preprocessed the data, defined a neural network model, trained it, evaluated its performance, and visualized some of its predictions. This serves as a foundational example for anyone looking to get started with regression tasks using deep learning. As always, there's room for improvement, and one can experiment with different architectures, optimizers, and other hyperparameters to achieve better results. The visualization step provides a more granular view of the model's performance, helping to identify areas for further refinement."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
