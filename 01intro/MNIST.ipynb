{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Importing Necessary Libraries\n",
        "Before we begin, we need to import the necessary Python libraries and modules that will help us in matrix operations, data loading, and other mathematical functions."
      ],
      "metadata": {
        "id": "6Pj2-sc0O3hh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "n593-lthO2NZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Loading and Preprocessing the Data\n",
        "The MNIST dataset is available in many deep learning libraries. Here, we'll use Keras to load the dataset."
      ],
      "metadata": {
        "id": "UUAjHlaoO-AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load MNIST from server\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# training data\n",
        "# reshape and normalize input data\n",
        "x_train = x_train.reshape(x_train.shape[0], 1, 28*28)\n",
        "x_train = x_train.astype('float32')\n",
        "x_train /= 255\n",
        "# encode output which is a number in range [0,9] into a vector of size 10\n",
        "# e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "\n",
        "# same for test data\n",
        "x_test = x_test.reshape(x_test.shape[0], 1, 28*28)\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255\n",
        "y_test = np_utils.to_categorical(y_test)"
      ],
      "metadata": {
        "id": "I2RoaQe4PFxy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Activation and Loss Functions Definitions\n",
        "Activation functions introduce non-linearity into the network. In this implementation, the hyperbolic tangent (tanh) function is utilized due to its zero-centered output, which can help speed up convergence during training. Additionally, the Mean Squared Error (MSE) is employed as the loss function. It measures the average squared difference between the actual and predicted values, making it suitable for regression problems.\n"
      ],
      "metadata": {
        "id": "8dxxe-_3PHZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# activation function and its derivative\n",
        "def tanh(x):\n",
        "    return np.tanh(x);\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1-np.tanh(x)**2;\n",
        "\n",
        "# loss function and its derivative\n",
        "def mse(y_true, y_pred):\n",
        "    return np.mean(np.power(y_true-y_pred, 2));\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "    return 2*(y_pred-y_true)/y_true.size;"
      ],
      "metadata": {
        "id": "uJTHk6GEPOAA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Building the Neural Network Components\n",
        "The neural network consists of layers. Here, we define the fully connected layer and the activation layer."
      ],
      "metadata": {
        "id": "ghP0djlPRLvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Fully Connected Layer\n",
        "The Fully Connected Layer (often abbreviated as FC layer) is a standard layer type that is used in many neural network architectures. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset."
      ],
      "metadata": {
        "id": "31GK5-LKRTex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base class\n",
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.input = None\n",
        "        self.output = None\n",
        "\n",
        "    # computes the output Y of a layer for a given input X\n",
        "    def forward_propagation(self, input):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# inherit from base class Layer\n",
        "class FCLayer(Layer):\n",
        "    # input_size = number of input neurons\n",
        "    # output_size = number of output neurons\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
        "        self.bias = np.random.rand(1, output_size) - 0.5\n",
        "\n",
        "    # returns output for a given input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = np.dot(self.input, self.weights) + self.bias\n",
        "        return self.output\n",
        "\n",
        "    # computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights.T)\n",
        "        weights_error = np.dot(self.input.T, output_error)\n",
        "        # dBias = output_error\n",
        "\n",
        "        # update parameters\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error"
      ],
      "metadata": {
        "id": "rcZoKEGgRPIt"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Activation Layer\n",
        "The Activation Layer is responsible for applying an activation function to its inputs. Activation functions introduce non-linear properties to the network, allowing it to learn from the error and make adjustments, which is essential for learning complex patterns."
      ],
      "metadata": {
        "id": "6jm80j1JRbrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inherit from base class Layer\n",
        "class ActivationLayer(Layer):\n",
        "    def __init__(self, activation, activation_prime):\n",
        "        self.activation = activation\n",
        "        self.activation_prime = activation_prime\n",
        "\n",
        "    # returns the activated input\n",
        "    def forward_propagation(self, input_data):\n",
        "        self.input = input_data\n",
        "        self.output = self.activation(self.input)\n",
        "        return self.output\n",
        "\n",
        "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
        "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
        "    def backward_propagation(self, output_error, learning_rate):\n",
        "        return self.activation_prime(self.input) * output_error"
      ],
      "metadata": {
        "id": "QUI0Ea4GRb5r"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5 Constructing the Neural Network\n",
        "The `Network` class represents the structure and functionality of a neural network. Upon initialization, it sets up an empty list for layers and placeholders for the loss function and its derivative. The `add` method allows for the addition of layers to the network, while the `use` method sets the loss function and its derivative to be used during training. The `predict` method performs forward propagation through the network for a given input, returning the network's output. The `fit` method trains the network using the provided training data. During training, the method iterates over the specified number of epochs, performing forward propagation to compute the network's output, calculating the loss, and then executing backward propagation to update the weights and biases of the layers. After each epoch, the average error across all samples is displayed."
      ],
      "metadata": {
        "id": "7xRLsW8KRhLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "        samples = len(input_data)\n",
        "        result = []\n",
        "\n",
        "        # run network over all samples\n",
        "        for i in range(samples):\n",
        "            # forward propagation\n",
        "            output = input_data[i]\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward_propagation(output)\n",
        "            result.append(output)\n",
        "\n",
        "        return result\n",
        "\n",
        "    # train the network\n",
        "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                    output = layer.forward_propagation(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward_propagation(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "            err /= samples\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))"
      ],
      "metadata": {
        "id": "TyCX_dleRkSv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 Training the Neural Network\n",
        "Training involves feeding the data into the network and adjusting the weights using backpropagation."
      ],
      "metadata": {
        "id": "aaJEFopyRvk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Network\n",
        "net = Network()\n",
        "net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)\n",
        "net.add(ActivationLayer(tanh, tanh_prime))\n",
        "\n",
        "# train on 1000 samples\n",
        "# as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...\n",
        "net.use(mse, mse_prime)\n",
        "net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBBDjRSURzQY",
        "outputId": "c669261e-dbba-48e8-ff9c-56cec70c54f8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/35   error=0.225384\n",
            "epoch 2/35   error=0.103824\n",
            "epoch 3/35   error=0.080902\n",
            "epoch 4/35   error=0.066233\n",
            "epoch 5/35   error=0.056656\n",
            "epoch 6/35   error=0.049281\n",
            "epoch 7/35   error=0.043336\n",
            "epoch 8/35   error=0.038188\n",
            "epoch 9/35   error=0.033800\n",
            "epoch 10/35   error=0.030063\n",
            "epoch 11/35   error=0.027024\n",
            "epoch 12/35   error=0.024651\n",
            "epoch 13/35   error=0.022683\n",
            "epoch 14/35   error=0.020999\n",
            "epoch 15/35   error=0.019545\n",
            "epoch 16/35   error=0.018324\n",
            "epoch 17/35   error=0.017207\n",
            "epoch 18/35   error=0.016211\n",
            "epoch 19/35   error=0.015293\n",
            "epoch 20/35   error=0.014477\n",
            "epoch 21/35   error=0.013789\n",
            "epoch 22/35   error=0.013225\n",
            "epoch 23/35   error=0.012712\n",
            "epoch 24/35   error=0.012270\n",
            "epoch 25/35   error=0.011825\n",
            "epoch 26/35   error=0.011389\n",
            "epoch 27/35   error=0.010992\n",
            "epoch 28/35   error=0.010478\n",
            "epoch 29/35   error=0.010119\n",
            "epoch 30/35   error=0.009660\n",
            "epoch 31/35   error=0.009361\n",
            "epoch 32/35   error=0.008923\n",
            "epoch 33/35   error=0.008660\n",
            "epoch 34/35   error=0.008282\n",
            "epoch 35/35   error=0.008091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 Evaluating the Neural Network\n",
        "After training, it's essential to evaluate the model's performance on unseen data."
      ],
      "metadata": {
        "id": "tRv9rCkyRzYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test on 3 samples\n",
        "out = net.predict(x_test[0:3])\n",
        "print(\"\\n\")\n",
        "print(\"predicted values : \")\n",
        "print(out, end=\"\\n\")\n",
        "print(\"true values : \")\n",
        "print(y_test[0:3])"
      ],
      "metadata": {
        "id": "e27KvRktR2Ol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2e395ef-d3f0-4b8b-e4b8-6aa5d7c84f65"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "predicted values : \n",
            "[array([[-0.00348004,  0.01409909,  0.00895138,  0.0028811 ,  0.02987997,\n",
            "         0.00805698,  0.02639108,  0.97298983,  0.03382498, -0.00432507]]), array([[ 2.41093956e-02, -2.73723194e-02,  7.99137300e-01,\n",
            "         1.49083206e-01, -1.57058038e-04,  1.77398106e-01,\n",
            "         1.42346556e-02, -5.61381684e-02,  3.40053372e-01,\n",
            "         2.75526989e-01]]), array([[-0.02143487,  0.97517556,  0.03774701, -0.00111179,  0.03364395,\n",
            "         0.00542629,  0.02291317, -0.0200364 ,  0.05049729, -0.0074744 ]])]\n",
            "true values : \n",
            "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8 Conclusion\n",
        "Using a simple fully connected neural network, we can achieve a decent accuracy on the MNIST dataset. For state-of-the-art results, more advanced techniques like Convolutional Neural Networks (CNNs) are typically used."
      ],
      "metadata": {
        "id": "RADXx0RJR7jP"
      }
    }
  ]
}