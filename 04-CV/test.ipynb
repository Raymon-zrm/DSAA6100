{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bachpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define MSE criterion\n",
    "class nn_MSECriterion:\n",
    "    def forward(self, prediction, target):\n",
    "        return np.sum(np.square(prediction - target)) \n",
    "    \n",
    "    def backward(self, prediction, target):\n",
    "        return 2 * (prediction - target)\n",
    "    \n",
    "# define loss function\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x, grad_output):\n",
    "        return np.multiply(self.forward(x) * (1 - self.forward(x)), grad_output)\n",
    "    \n",
    "# define neural network\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.random.randn(output_size)\n",
    "        self.grad_weights = np.zeros_like(self.weights) * 0.01\n",
    "        self.grad_bias = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weights) + self.bias\n",
    "    \n",
    "    def backward(self, x, grad_output):\n",
    "        self.grad_weights = np.dot(x.T, grad_output)\n",
    "        self.grad_bias = np.copy(grad_output)\n",
    "        return np.dot(grad_output, self.weights.T)\n",
    "    \n",
    "    def get_params(self):\n",
    "        params = [self.weights, self.bias]\n",
    "        grad_params = [self.grad_weights, self.grad_bias]\n",
    "        return params, grad_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test some dummy inputs for a full pass of forward and backward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y1 = np.array([[0.25, 0.25, 0.25]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(4, 3)  # h(W, b)\n",
    "sigmoid = nn_Sigmoid()  # g(v)\n",
    "loss = nn_MSECriterion()  # f(u)\n",
    "\n",
    "# Forward-propagation.\n",
    "lin = linear.forward(x1)\n",
    "y_hat = sigmoid.forward(lin)\n",
    "loss_val = loss.forward(y_hat, y1) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "dy_hat = loss.backward(y_hat, y1)\n",
    "dlin = sigmoid.backward(lin, dy_hat)\n",
    "dx1 = linear.backward(x1, dlin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradWeight: [[ -3.18346091  -2.34519463  -3.43486793]\n",
      " [ -5.16972285  -3.80843573  -5.57799065]\n",
      " [-46.85401443 -34.51645431 -50.55421004]\n",
      " [ -7.25121652  -5.34183221  -7.82386584]]\n",
      "\n",
      "approxGradWeight: [[ -3.18347075  -2.34538141  -3.43461622]\n",
      " [ -5.16974876  -3.8089283   -5.57732685]\n",
      " [-46.85610065 -34.55692648 -50.4996925 ]\n",
      " [ -7.25126744  -5.34280131  -7.8225599 ]]\n"
     ]
    }
   ],
   "source": [
    "# We will compute derivatives with respect to a single data pair (x,y)\n",
    "x = np.array([[2.34, 3.8, 34.44, 5.33]])\n",
    "y = np.array([[3.2, 4.2, 5.3]])\n",
    "\n",
    "hidden_state_size = 5;\n",
    "\n",
    "model = {}\n",
    "model['linear1'] = nn_Linear(4, hidden_state_size)  # 第一个线性层，输出维度改为5\n",
    "model['sigmoid1'] = nn_Sigmoid()    # 第一个 Sigmoid 激活层\n",
    "model['linear2'] = nn_Linear(hidden_state_size, 3)  # 第二个线性层，输入维度为5，输出维度为3\n",
    "model['sigmoid2'] = nn_Sigmoid()    # 第二个 Sigmoid 激活层\n",
    "model['loss'] = nn_MSECriterion()\n",
    "\n",
    "gradWeight1 = model['linear1'].grad_weights\n",
    "gradBias1 = model['linear1'].grad_bias\n",
    "gradWeight2 = model['linear2'].grad_weights\n",
    "gradBias2 = model['linear2'].grad_bias\n",
    "\n",
    "approxGradWeight1 = np.zeros_like(model['linear1'].weights)\n",
    "approxGradBias1 = np.zeros_like(model['linear1'].bias)\n",
    "approxGradWeight2 = np.zeros_like(model['linear2'].weights)\n",
    "approxGradBias2 = np.zeros_like(model['linear2'].bias)\n",
    "\n",
    "epsilon = 1e-4\n",
    "for i in range(0, linear.weights.shape[0]):\n",
    "    for j in range(0, linear.weights.shape[1]):\n",
    "        # Forward-propagation.\n",
    "        a0 = model['linear1'].forward(x)\n",
    "        a1 = model['sigmoid1'].forward(a0)  # 第一个 Sigmoid 激活层\n",
    "        a2 = model['linear2'].forward(a1)   # 第二个线性层\n",
    "        a3 = model['sigmoid2'].forward(a2)  # 第二个 Sigmoid 激活层\n",
    "        loss1 = model['loss'].forward(a3, y)\n",
    "        shift_weight1 = np.copy(model['linear1'].weights)\n",
    "        shift_weight2 = np.copy(model['linear2'].weights)\n",
    "        shift_weight1[i, j] += epsilon\n",
    "        shift_weight2[i, j] += epsilon\n",
    "        shift_linear1 = nn_Linear(4, hidden_state_size)\n",
    "        shift_linear1.bias = model['linear1'].bias\n",
    "        shift_linear1.weights = shift_weight1\n",
    "        shift_linear2 = nn_Linear(hidden_state_size, 3)\n",
    "        shift_linear2.bias = model['linear2'].bias\n",
    "        shift_linear2.weights = shift_weight2\n",
    "        shift_a0 = shift_linear1.forward(x)\n",
    "        shift_a1 = sigmoid.forward(shift_a0)\n",
    "        shift_a2 = shift_linear2.forward(shift_a1)\n",
    "        shift_a3 = sigmoid.forward(shift_a2)\n",
    "        loss2 = model['loss'].forward(shift_a3, y)\n",
    "        approxGradWeight[i, j] = (loss2 - loss1) / epsilon\n",
    "        \n",
    "# These two outputs should be similar up to some precision.\n",
    "print('gradWeight: ' + str(gradWeight))\n",
    "print('\\napproxGradWeight: ' + str(approxGradWeight))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
