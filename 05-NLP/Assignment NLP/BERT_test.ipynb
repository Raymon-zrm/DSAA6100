{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Import packages and data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raymon/miniconda3/envs/colab/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Process data\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Train model\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "movie_review = pd.read_csv('./movie_reviews/movie_reviews.csv')\n",
    "test_dataset = pd.read_csv(\"./test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Describe and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset:\n",
      "                                                text  label\n",
      "0  If you havent seen this movie than you need to...      1\n",
      "1  but Cinderella gets my vote not only for the w...      0\n",
      "2  This movie is pretty cheesy but I do give it c...      1\n",
      "3  I have not seen a Van Damme flick for a while ...      1\n",
      "4  This is a sleeper It defines Nicholas Cage The...      1\n",
      "\n",
      "test dataset:\n",
      "   Id                                               text\n",
      "0   0  What can possibly said about this movie other ...\n",
      "1   1  I dont care how many bad reviews purple rain g...\n",
      "2   2  Ken Russell directed this weird  Not very  ero...\n",
      "3   3  This is a great movie from the lost age of rea...\n",
      "4   4  I have a problem with the movie snobs who cons...\n"
     ]
    }
   ],
   "source": [
    "# 1. Check the train dataset and test dataset\n",
    "print(f'train dataset:\\n{movie_review.head()}\\n')\n",
    "print(f'test dataset:\\n{test_dataset.head()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Clean the train dataset and test dataset\n",
    "\n",
    "# Function to clean the text data\n",
    "def clean_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove leading and trailing spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "movie_review['text_cleaned'] = movie_review['text'].apply(clean_text)\n",
    "test_dataset['text_cleaned'] = test_dataset['text'].apply(clean_text)\n",
    "\n",
    "train_dataset = movie_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 Process data with DistilBertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 14:47:10.360226: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-08 14:47:10.391754: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-08 14:47:10.537015: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-08 14:47:10.537035: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-08 14:47:10.537924: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-08 14:47:10.615192: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-08 14:47:11.268140: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# In the beginning I use the standard ways to train the model, but the result is not good enough.\n",
    "# Then I tried to use the BERT model and BiLSTM model, but encountered the problem of overfitting, so I changed to DistilBERT model.\n",
    "\n",
    "# Load the DistilBERT tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Function to encode the text data into tokens\n",
    "def encode_reviews(reviews, labels, max_length):\n",
    "    return tokenizer(reviews, truncation=True, padding='max_length', max_length=max_length, return_tensors=\"pt\")\n",
    "\n",
    "# Encode the training set\n",
    "max_length = 128 # Maximum length of a review\n",
    "encoded_train_data = encode_reviews(train_dataset['text_cleaned'].tolist(), train_dataset['label'].tolist(), max_length)\n",
    "\n",
    "# Split the training set into train and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_dataset['text_cleaned'], train_dataset['label'], test_size=0.2)\n",
    "\n",
    "# Encode the train and validation sets\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Create a Dataset object\n",
    "class MovieReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Convert the dataset into a Dataset object\n",
    "train_dataset = MovieReviewsDataset(train_encodings, train_labels.tolist())                                      \n",
    "val_dataset = MovieReviewsDataset(val_encodings, val_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 Train a DistilBert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/raymon/miniconda3/envs/colab/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6000' max='6000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6000/6000 03:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.323500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.208800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.184600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.092200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.087000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.txt',\n",
       " './saved_model/added_tokens.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "# Load the DistilBERT model with a classification head\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "\n",
    "# Set the device to GPU (cuda) if available, otherwise stick with CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset             \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained('./saved_model')\n",
    "tokenizer.save_pretrained('./saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1/125 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46378642320632935, 'eval_accuracy': 0.8935, 'eval_f1': 0.8953574060427413, 'eval_precision': 0.8883743602242262, 'eval_recall': 0.9024511017578608, 'eval_runtime': 3.1184, 'eval_samples_per_second': 2565.432, 'eval_steps_per_second': 40.085}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Define the evaluation metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,           # accuracy_score(labels, preds)\n",
    "        'f1': f1,                  # f1_score(labels, preds)\n",
    "        'precision': precision,    # precision_score(labels, preds)\n",
    "        'recall': recall           # recall_score(labels, preds)\n",
    "    }\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "evaluation_result = trainer.evaluate()\n",
    "\n",
    "print(evaluation_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 Predict the testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model_path = 'saved_model'\n",
    "\n",
    "# Load the DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path).to('cuda')  \n",
    "\n",
    "test_data = test_dataset['text_cleaned'].tolist()  \n",
    "\n",
    "# Process the testdata in several batches\n",
    "batch_size = 100\n",
    "\n",
    "# Store the predicted labels and scores\n",
    "predicted_labels = []\n",
    "predicted_scores = []\n",
    "\n",
    "# Process the test data in batches\n",
    "for i in range(0, len(test_data), batch_size):\n",
    "    batch = test_data[i:i + batch_size]  \n",
    "    inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\").to('cuda')  # 处理数据并移到GPU上\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # convert logits to probabilities\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "\n",
    "    # obtain the predicted labels and scores\n",
    "    predictions = torch.argmax(probabilities, dim=1)\n",
    "    scores = probabilities[torch.arange(probabilities.size(0)), predictions]\n",
    "\n",
    "    # put the predicted labels and scores to the lists\n",
    "    predicted_labels.extend(predictions.tolist())\n",
    "    predicted_scores.extend(scores.tolist())\n",
    "    \n",
    "\n",
    "# Add the predicted labels and scores to the test dataset\n",
    "test_dataset['predicted_label'] = predicted_labels\n",
    "test_dataset = test_dataset.drop(columns=['text_cleaned','text'])\n",
    "test_dataset.to_csv('test_data_with_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
